# /data/agirard/Projects/TimeTravel-PolicyGradientRL/src/utils/metrics.py

import logging
from sacrebleu.metrics import BLEU
from rouge import Rouge
from bert_score import BERTScorer
from src.utils.config import CONFIG

logger = logging.getLogger(__name__)

class MetricsEvaluator:
    """
    A class for evaluating text generation models using various metrics such as BLEU, ROUGE, and BERTScore.
    This class helps compute rewards based on selected metrics for reinforcement learning methods like policy gradient.
    """

    def __init__(self):
        """
        Initializes the metric evaluators based on the configurations provided in CONFIG.
        Depending on the settings, it initializes evaluators for BLEU, ROUGE, and BERTScore.
        """
        print(f"Initializing MetricsEvaluator with config: {CONFIG}")
        
        # Initialize ROUGE for ROUGE score calculation
        self.rouge = Rouge()  

        # Optionally initialize BERTScorer if configured to use BERTScore
        if CONFIG.get("use_bert", False):
            self.bert_scorer = BERTScorer(
                model_type=CONFIG["bert_scorer_model_type"],  # The BERT model type to use
                device=CONFIG["scorer_device"],  # The device (GPU/CPU) to use for evaluation
                num_layers=None,  # Optionally specify number of layers; default to all layers
                batch_size=CONFIG["bert_scorer_batch_size"]  # Batch size for evaluation
            )
        else:
            # If BERTScore is not to be used, set bert_scorer to None
            self.bert_scorer = None

        print("MetricsEvaluator initialized.")

    def calculate_reward(self, generated_texts, references):
        """
        Calculates the reward based on the specified metric in CONFIG.
        This method supports ROUGE-L and BERTScore as the reward mechanisms.
        
        Args:
            generated_texts (list of str): Texts generated by the model.
            references (list of str): Reference texts (ground truth).

        Returns:
            rewards (list of float): A list of reward values per example.
        """
        reward_metric = CONFIG.get("reward_metric", "rouge")

        # Debugging prints
        print("DEBUG: Type of generated_texts:", type(generated_texts))
        print("DEBUG: generated_texts:", generated_texts)
        print("DEBUG: Type of references:", type(references))
        print("DEBUG: references:", references)

        # Ensure both generated_texts and references are lists
        if isinstance(generated_texts, tuple):
            generated_texts = list(generated_texts)
        if isinstance(references, tuple):
            references = list(references)

        # Ensure that the elements are strings
        generated_texts = [str(gt) for gt in generated_texts]
        references = [str(ref) for ref in references]

        # Debugging prints after processing
        print("DEBUG: Type of generated_texts after processing:", type(generated_texts))
        print("DEBUG: generated_texts after processing:", generated_texts)
        print("DEBUG: Type of references after processing:", type(references))
        print("DEBUG: references after processing:", references)

        # Case 1: ROUGE-L is selected as the reward metric
        if reward_metric == "rouge":
            # Calculate ROUGE-L F1 score between generated and reference texts
            rouge_scores = self.rouge.get_scores(generated_texts, references, avg=False)
            rewards = [score['rouge-l']['f'] for score in rouge_scores]  # List of rewards per example

        # Case 2: BERTScore is selected as the reward metric
        elif reward_metric == "bert":
            if self.bert_scorer is None:
                raise ValueError("BERTScore is not initialized. Set 'use_bert' to True in CONFIG.")
            _, _, f1 = self.bert_scorer.score(generated_texts, references)
            rewards = f1.tolist()  # Convert tensor to list

        # If the reward metric specified is unsupported, raise an error
        else:
            raise ValueError(f"Unsupported reward metric: {reward_metric}")

        return rewards

    def calculate_and_log_bleu_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs SacreBLEU scores for various comparisons between generated texts and references.
        
        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging BLEU score information

        Returns:
        - bleu_scores: Dictionary with calculated BLEU scores for different comparisons
        """
        print("Calculating BLEU scores...")

        # Prepare references for BLEU score calculation
        edited_endings_refs = [[ending] for ending in all_edited_endings] if all_edited_endings else None
        counterfactuals_refs = [[cf] for cf in all_counterfactuals]
        initials_refs = [[init] for init in all_initials]
        original_endings_refs = [[orig] for orig in all_original_endings]

        # List of all comparisons we want to calculate BLEU scores for
        all_comparisons = [
            ('bleu_prediction_edited', all_generated_texts, edited_endings_refs),
            ('bleu_prediction_cf', all_generated_texts, counterfactuals_refs),
            ('bleu_prediction_initial', all_generated_texts, initials_refs),
            ('bleu_prediction_original', all_generated_texts, original_endings_refs),
            ('bleu_edited_ending_cf', all_edited_endings, counterfactuals_refs),
            ('bleu_edited_ending_initial', all_edited_endings, initials_refs),
            ('bleu_edited_ending_original', all_edited_endings, original_endings_refs),
        ]

        # Dictionary to store BLEU scores for each comparison
        bleu_scores = {}
        for label, texts, references in all_comparisons:
            if references is not None:
                try:
                    # Calculate BLEU score
                    bleu_result = self.sacre_bleu.corpus_score(texts, references)
                    bleu_score = bleu_result.score
                    logger.info(f"{label}: {bleu_score}")
                    bleu_scores[label] = bleu_score
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bleu_scores[label] = 'N/A'

        return bleu_scores

    def calculate_and_log_rouge_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs ROUGE scores for various comparisons between generated texts and references.

        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging ROUGE score information

        Returns:
        - rouge_scores: Dictionary with calculated ROUGE-L F1 scores for different comparisons
        """
        print("Calculating ROUGE scores...")

        # List of all comparisons we want to calculate ROUGE-L F1 scores for
        all_comparisons = [
            ('rouge_prediction_edited', all_generated_texts, all_edited_endings),
            ('rouge_prediction_cf', all_generated_texts, all_counterfactuals),
            ('rouge_prediction_initial', all_generated_texts, all_initials),
            ('rouge_prediction_original', all_generated_texts, all_original_endings),
            ('rouge_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('rouge_edited_ending_initial', all_edited_endings, all_initials),
            ('rouge_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        # Dictionary to store ROUGE-L F1 scores for each comparison
        rouge_scores = {}
        for label, hypotheses, references in all_comparisons:
            if references:
                try:
                    # Calculate ROUGE-L score
                    rouge_scores_set = self.rouge.get_scores(hypotheses, references, avg=True)
                    score_type = 'rouge-l'
                    rouge_scores[f"{label}_{score_type}_f"] = rouge_scores_set[score_type]['f']
                    logger.info(f"{label}_{score_type}_f: {rouge_scores_set[score_type]['f']}")
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    rouge_scores[f"{label}_f"] = 'N/A'

        return rouge_scores

    def calculate_and_log_bert_similarity(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs BERT similarity F1 scores for various comparisons of generated texts and references.

        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging BERTScore information

        Returns:
        - bert_scores: Dictionary with calculated BERT F1 scores for different comparisons
        """
        print("Calculating BERT similarity F1 scores...")

        all_comparisons = [
            ('bert_prediction_edited', all_generated_texts, all_edited_endings),
            ('bert_prediction_cf', all_generated_texts, all_counterfactuals),
            ('bert_prediction_initial', all_generated_texts, all_initials),
            ('bert_prediction_original', all_generated_texts, all_original_endings),
            ('bert_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('bert_edited_ending_initial', all_edited_endings, all_initials),
            ('bert_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        # Dictionary to store BERT F1 scores for each comparison
        bert_scores = {}
        for label, texts_a, texts_b in all_comparisons:
            if texts_b:
                try:
                    # Calculate BERTScore F1
                    _, _, f1 = self.bert_scorer.score(texts_a, texts_b)
                    avg_f1 = f1.mean().item()
                    logger.info(f"{label}_f1: {avg_f1}")
                    bert_scores[f"{label}_f1"] = avg_f1
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bert_scores[f"{label}_f1"] = 'N/A'

        return bert_scores
