# /data/agirard/Projects/TimeTravel-PolicyGradientRL/src/utils/metrics.py
import logging
from sacrebleu.metrics import BLEU
from rouge import Rouge
from bert_score import BERTScorer
import torch
from src.utils.config import CONFIG
from src.BARTScore_metric.bart_score import BARTScorer 

logger = logging.getLogger(__name__)

class MetricsEvaluator:
    """
    A class for evaluating text generation models using various metrics such as BLEU, ROUGE, BERTScore, and BARTScore.
    This class helps compute rewards based on selected metrics for reinforcement learning methods like policy gradient.
    """

    def __init__(self):
        """
        Initializes the metric evaluators based on the configurations provided in CONFIG.
        Depending on the settings, it initializes evaluators for BLEU, ROUGE, BERTScore, and BARTScore.
        """
        print(f"Initializing MetricsEvaluator with config: {CONFIG}")
        
        # Initialize ROUGE for ROUGE score calculation
        self.rouge = Rouge()  

        # Initialize BERTScorer if configured to use BERTScore
        if CONFIG.get("use_bert", False):
            self.bert_scorer = BERTScorer(
                model_type=CONFIG["bert_scorer_model_type"],  # The BERT model type to use
                device=CONFIG["scorer_device"],  # Device for BERT scorer
                num_layers=None,  # Optionally specify number of layers; default to all layers
                batch_size=CONFIG["bert_scorer_batch_size"]  # Batch size for evaluation
            )
        else:
            self.bert_scorer = None  # Set bert_scorer to None if BERTScore is not used

        # Initialize BLEU scorer
        self.sacre_bleu = BLEU() if CONFIG.get("use_bleu", False) else None

        # Initialize BARTScorer if configured to use BARTScore
        if CONFIG.get("use_bart", False):
            self.bart_scorer = BARTScorer(device=CONFIG["scorer_device"], checkpoint=CONFIG["bart_scorer_checkpoint"])
        else:
            self.bart_scorer = None

        print("MetricsEvaluator initialized.")

    def calculate_reward(self, generated_texts, references):
        """
        Calculates the reward based on the specified metric in CONFIG.
        This method supports ROUGE-L, BERTScore, BLEU, and BARTScore as the reward mechanisms.
        
        Args:
            generated_texts (list of str): Texts generated by the model.
            references (list of str): Reference texts (ground truth).

        Returns:
            rewards (tensor): A tensor of reward values per example on GPU 0.
        """
        reward_metric = CONFIG.get("reward_metric", "rouge")

        print(f"Reward metric: {reward_metric}")  # Print the reward metric being used
        print(f"Generated texts: {generated_texts}")  # Print the generated texts
        print(f"Reference texts: {references}")  # Print the reference (ground truth) texts

        # Ensure both generated_texts and references are lists of strings
        generated_texts = [str(gt) for gt in generated_texts]
        references = [str(ref) for ref in references]

        # Case 1: ROUGE-L is selected as the reward metric
        if reward_metric == "rouge":
            print("Calculating ROUGE-L scores...")  # Indicate that ROUGE-L is being calculated
            rouge_scores = self.rouge.get_scores(generated_texts, references, avg=False)
            rewards = [score['rouge-l']['f'] for score in rouge_scores]  # List of rewards per example

        # Case 2: BERTScore is selected as the reward metric
        elif reward_metric == "bert":
            if self.bert_scorer is None:
                raise ValueError("BERTScore is not initialized. Set 'use_bert' to True in CONFIG.")
            print("Calculating BERTScore...")  # Indicate that BERTScore is being calculated
            _, _, f1 = self.bert_scorer.score(generated_texts, references)
            rewards = f1.tolist()  # Convert tensor to list

        # Case 3: BLEU is selected as the reward metric
        elif reward_metric == "bleu":
            if self.sacre_bleu is None:
                raise ValueError("BLEU is not initialized. Set 'use_bleu' to True in CONFIG.")
            print("Calculating BLEU score...")  # Indicate that BLEU is being calculated
            bleu_result = self.sacre_bleu.corpus_score(generated_texts, [references])
            rewards = [bleu_result.score for _ in generated_texts]  # Use the BLEU score for all samples

        # Case 4: BARTScore is selected as the reward metric
        elif reward_metric == "bart":
            if self.bart_scorer is None:
                raise ValueError("BARTScore is not initialized. Set 'use_bart' to True in CONFIG.")
            print("Calculating BARTScore...")  # Indicate that BARTScore is being calculated
            rewards = self.bart_scorer.score(generated_texts, references).tolist()  # BARTScore for each example

        # Raise an error if an unsupported reward metric is specified
        else:
            raise ValueError(f"Unsupported reward metric: {reward_metric}")

        # Convert the rewards to a tensor and move it to the correct device (GPU 0)
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to(CONFIG["scorer_device"])
        print(f"Rewards tensor (on {CONFIG['scorer_device']}): {rewards_tensor}")  # Print the final rewards tensor
        return rewards_tensor


    def calculate_and_log_bleu_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs SacreBLEU scores for various comparisons between generated texts and references.
        
        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging BLEU score information

        Returns:
        - bleu_scores: Dictionary with calculated BLEU scores for different comparisons
        """
        print("Calculating BLEU scores...")

        # Prepare references for BLEU score calculation
        edited_endings_refs = [[ending] for ending in all_edited_endings] if all_edited_endings else None
        counterfactuals_refs = [[cf] for cf in all_counterfactuals]
        initials_refs = [[init] for init in all_initials]
        original_endings_refs = [[orig] for orig in all_original_endings]

        # List of all comparisons we want to calculate BLEU scores for
        all_comparisons = [
            ('bleu_prediction_edited', all_generated_texts, edited_endings_refs),
            ('bleu_prediction_cf', all_generated_texts, counterfactuals_refs),
            ('bleu_prediction_initial', all_generated_texts, initials_refs),
            ('bleu_prediction_original', all_generated_texts, original_endings_refs),
            ('bleu_edited_ending_cf', all_edited_endings, counterfactuals_refs),
            ('bleu_edited_ending_initial', all_edited_endings, initials_refs),
            ('bleu_edited_ending_original', all_edited_endings, original_endings_refs),
        ]

        # Dictionary to store BLEU scores for each comparison
        bleu_scores = {}
        for label, texts, references in all_comparisons:
            if references is not None:
                try:
                    # Calculate BLEU score
                    bleu_result = self.sacre_bleu.corpus_score(texts, references)
                    bleu_score = bleu_result.score
                    logger.info(f"{label}: {bleu_score}")
                    bleu_scores[label] = bleu_score
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bleu_scores[label] = 'N/A'

        return bleu_scores

    def calculate_and_log_rouge_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs ROUGE scores for various comparisons between generated texts and references.

        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging ROUGE score information

        Returns:
        - rouge_scores: Dictionary with calculated ROUGE-L F1 scores for different comparisons
        """
        print("Calculating ROUGE scores...")

        # List of all comparisons we want to calculate ROUGE-L F1 scores for
        all_comparisons = [
            ('rouge_prediction_edited', all_generated_texts, all_edited_endings),
            ('rouge_prediction_cf', all_generated_texts, all_counterfactuals),
            ('rouge_prediction_initial', all_generated_texts, all_initials),
            ('rouge_prediction_original', all_generated_texts, all_original_endings),
            ('rouge_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('rouge_edited_ending_initial', all_edited_endings, all_initials),
            ('rouge_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        # Dictionary to store ROUGE-L F1 scores for each comparison
        rouge_scores = {}
        for label, hypotheses, references in all_comparisons:
            if references:
                try:
                    # Calculate ROUGE-L score
                    rouge_scores_set = self.rouge.get_scores(hypotheses, references, avg=True)
                    score_type = 'rouge-l'
                    rouge_scores[f"{label}_{score_type}_f"] = rouge_scores_set[score_type]['f']
                    logger.info(f"{label}_{score_type}_f: {rouge_scores_set[score_type]['f']}")
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    rouge_scores[f"{label}_f"] = 'N/A'

        return rouge_scores

    def calculate_and_log_bert_similarity(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs BERT similarity F1 scores for various comparisons of generated texts and references.

        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging BERTScore information

        Returns:
        - bert_scores: Dictionary with calculated BERT F1 scores for different comparisons
        """
        print("Calculating BERT similarity F1 scores...")

        all_comparisons = [
            ('bert_prediction_edited', all_generated_texts, all_edited_endings),
            ('bert_prediction_cf', all_generated_texts, all_counterfactuals),
            ('bert_prediction_initial', all_generated_texts, all_initials),
            ('bert_prediction_original', all_generated_texts, all_original_endings),
            ('bert_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('bert_edited_ending_initial', all_edited_endings, all_initials),
            ('bert_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        # Dictionary to store BERT F1 scores for each comparison
        bert_scores = {}
        for label, texts_a, texts_b in all_comparisons:
            if texts_b:
                try:
                    # Calculate BERTScore F1
                    _, _, f1 = self.bert_scorer.score(texts_a, texts_b)
                    avg_f1 = f1.mean().item()
                    logger.info(f"{label}_f1: {avg_f1}")
                    bert_scores[f"{label}_f1"] = avg_f1
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bert_scores[f"{label}_f1"] = 'N/A'

        return bert_scores

    def calculate_and_log_bart_similarity(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs BART-based similarity scores for a variety of text comparisons,
        using the BARTScorer to evaluate the similarity between different segments of texts.
        This version only supports single-reference scoring.
        """
        print("Calculating BART similarity scores...")
        
        # Define all pairs of text segments for which to calculate similarity scores
        all_comparisons = [
            ('bart_prediction_edited', all_generated_texts, all_edited_endings),
            ('bart_prediction_cf', all_generated_texts, all_counterfactuals),
            ('bart_prediction_initial', all_generated_texts, all_initials),
            ('bart_prediction_original', all_generated_texts, all_original_endings),
            ('bart_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('bart_edited_ending_initial', all_edited_endings, all_initials),
            ('bart_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        print(f"BART Comparisons: {all_comparisons}")

        # Calculate and log BARTScores for each comparison
        bart_scores = {}
        for label, src_texts, tgt_texts in all_comparisons:
            if tgt_texts:
                try:
                    # Single-reference scoring
                    scores = self.bart_scorer.score(src_texts, tgt_texts, batch_size=4)

                    # Validate that the number of scores matches the number of source texts
                    if len(scores) != len(src_texts):
                        raise ValueError(f"Mismatch in the number of scores returned. Expected {len(src_texts)} but got {len(scores)}.")

                    avg_score = sum(scores) / len(scores) if scores else float('nan')
                    logger.info(f"{label}_avg_score: {avg_score}")
                    bart_scores[f"{label}_avg_score"] = avg_score
                    print(f"{label}_avg_score: {avg_score}")
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bart_scores[f"{label}_avg_score"] = 'N/A'
                    print(f"Error calculating {label}: {e}")
        return bart_scores