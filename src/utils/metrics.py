import logging
from sacrebleu.metrics import BLEU
from rouge import Rouge
from bert_score import BERTScorer
import torch
from src.utils.config import CONFIG
from src.BARTScore_metric.bart_score import BARTScorer

logger = logging.getLogger(__name__)

class MetricsEvaluator:
    """
    A class for evaluating text generation models using various metrics such as BLEU, ROUGE, BERTScore, and BARTScore.
    This class helps compute scores for supervised learning and rewards for reinforcement learning.
    """

    def __init__(self):
        """
        Initializes the metric evaluators based on the configurations provided in CONFIG.
        Depending on the settings, it initializes evaluators for BLEU, ROUGE, BERTScore, and BARTScore.
        """
        print(f"Initializing MetricsEvaluator with config: {CONFIG}")
        
        # Initialize ROUGE for ROUGE score calculation
        self.rouge = Rouge()  

        # Initialize BERTScorer if configured to use BERTScore
        self.bert_scorer = BERTScorer(
            model_type=CONFIG.get("bert_scorer_model_type", "bert-base-uncased"),
            device=CONFIG.get("scorer_device", "cpu"),
            num_layers=CONFIG.get("bert_scorer_num_layers", None),
            batch_size=CONFIG.get("bert_scorer_batch_size", 16)
        ) if CONFIG.get("use_bert", False) else None

        # Initialize BLEU scorer if configured to use BLEU
        self.sacre_bleu = BLEU() if CONFIG.get("use_bleu", False) else None

        # Check if sacrebleu supports effective_order
        self.supports_effective_order = hasattr(self.sacre_bleu, 'sentence_score') and \
                                        'effective_order' in BLEU.sentence_score.__code__.co_varnames

        # Initialize BARTScorer if configured to use BARTScore
        self.bart_scorer = BARTScorer(
            device=CONFIG.get("scorer_device", "cpu"), 
            checkpoint=CONFIG.get("bart_scorer_checkpoint", "facebook/bart-large-cnn")
        ) if CONFIG.get("use_bart", False) else None

        print("MetricsEvaluator initialized.")

    def calculate_score(self, generated_texts, references):
        """
        Calculates the score based on the specified metric in CONFIG.
        This method supports ROUGE-L, BERTScore, BLEU, and BARTScore as scoring mechanisms.
        
        Args:
            generated_texts (list of str): Texts generated by the model.
            references (list of str): Reference texts (ground truth).

        Returns:
            scores_tensor (torch.Tensor): A tensor of score values per example on the specified device.
        """
        score_metric = CONFIG.get("reward_metric", "rouge")
        scorer_device = CONFIG.get("scorer_device", "cpu")

        # Debugging prints for input and metric selection
        print(f"Score metric: {score_metric}")
        print(f"Generated texts: {generated_texts}")
        print(f"Reference texts: {references}")

        # Ensure inputs are lists of strings
        generated_texts = [str(gt) for gt in generated_texts]
        references = [str(ref) for ref in references]

        # Case 1: ROUGE-L is selected as the score metric
        if score_metric == "rouge":
            print("Calculating ROUGE-L scores...")
            rouge_scores = self.rouge.get_scores(generated_texts, references, avg=False)
            scores = [score['rouge-l']['f'] for score in rouge_scores]  # Extract ROUGE-L F1 scores

        # Case 2: BERTScore is selected as the score metric
        elif score_metric == "bert":
            if self.bert_scorer is None:
                raise ValueError("BERTScore is not initialized. Set 'use_bert' to True in CONFIG.")
            print("Calculating BERTScore...")
            _, _, f1 = self.bert_scorer.score(generated_texts, references)
            scores = f1.tolist()  # Convert tensor to list

        # Case 3: BLEU is selected as the score metric with one-to-one comparison
        elif score_metric == "bleu":
            if self.sacre_bleu is None:
                raise ValueError("BLEU is not initialized. Set 'use_bleu' to True in CONFIG.")
            print("Calculating BLEU score for each generated-reference pair...")
            scores = []
            for gen_text, ref_text in zip(generated_texts, references):
                score = self.sacre_bleu.sentence_score(gen_text, [ref_text]).score
                scores.append(score)

        # Case 4: BARTScore is selected as the score metric
        elif score_metric == "bart":
            if self.bart_scorer is None:
                raise ValueError("BARTScore is not initialized. Set 'use_bart' to True in CONFIG.")
            print("Calculating BARTScore...")
            scores = self.bart_scorer.score(generated_texts, references)  # BARTScore for each example

        # Unsupported score metric
        else:
            raise ValueError(f"Unsupported score metric: {score_metric}")

        # Convert scores to a tensor and move it to the specified device
        scores_tensor = torch.tensor(scores, dtype=torch.float32, device=scorer_device)
        print(f"Scores tensor (on {scorer_device}): {scores_tensor}")  # Print the final scores tensor
        return scores_tensor

    def calculate_and_log_bleu_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs SacreBLEU scores for various comparisons between generated texts and references.
        """
        print("Calculating BLEU scores...")

        # Prepare references for BLEU score calculation
        edited_endings_refs = all_edited_endings if all_edited_endings else None
        counterfactuals_refs = all_counterfactuals
        initials_refs = all_initials
        original_endings_refs = all_original_endings

        # List of all comparisons we want to calculate BLEU scores for
        all_comparisons = [
            ('bleu_prediction_edited', all_generated_texts, edited_endings_refs),
            ('bleu_prediction_cf', all_generated_texts, counterfactuals_refs),
            ('bleu_prediction_initial', all_generated_texts, initials_refs),
            ('bleu_prediction_original', all_generated_texts, original_endings_refs),
            ('bleu_edited_ending_cf', all_edited_endings, counterfactuals_refs),
            ('bleu_edited_ending_initial', all_edited_endings, initials_refs),
            ('bleu_edited_ending_original', all_edited_endings, original_endings_refs),
        ]

        # Dictionary to store BLEU scores for each comparison
        bleu_scores = {}
        for label, texts, references in all_comparisons:
            if references is not None:
                try:
                    # Compute BLEU score for each generated-reference pair individually
                    individual_scores = []
                    for gen_text, ref_text in zip(texts, references):
                        bleu_result = self.sacre_bleu.sentence_score(gen_text, [ref_text])
                        individual_scores.append(bleu_result.score)
                    
                    # Log individual BLEU scores for debugging purposes
                    logger.info(f"{label} individual BLEU scores: {individual_scores}")
                    
                    # Average BLEU scores for the current comparison
                    average_bleu_score = sum(individual_scores) / len(individual_scores) if individual_scores else float('nan')
                    logger.info(f"{label} average BLEU score: {average_bleu_score}")
                    bleu_scores[label] = {
                        "individual_scores": individual_scores,
                        "average_score": average_bleu_score
                    }

                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bleu_scores[label] = 'N/A'

        return bleu_scores

    def calculate_and_log_rouge_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs ROUGE scores for various comparisons between generated texts and references.
        """
        print("Calculating ROUGE scores...")

        all_comparisons = [
            ('rouge_prediction_edited', all_generated_texts, all_edited_endings),
            ('rouge_prediction_cf', all_generated_texts, all_counterfactuals),
            ('rouge_prediction_initial', all_generated_texts, all_initials),
            ('rouge_prediction_original', all_generated_texts, all_original_endings),
            ('rouge_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('rouge_edited_ending_initial', all_edited_endings, all_initials),
            ('rouge_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        rouge_scores = {}
        for label, hypotheses, references in all_comparisons:
            if references:
                try:
                    rouge_scores_set = self.rouge.get_scores(hypotheses, references, avg=True)
                    score_type = 'rouge-l'
                    rouge_scores[f"{label}_{score_type}_f"] = rouge_scores_set[score_type]['f']
                    logger.info(f"{label}_{score_type}_f: {rouge_scores_set[score_type]['f']}")
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    rouge_scores[f"{label}_f"] = 'N/A'

        return rouge_scores

    def calculate_and_log_bert_similarity(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs BERT similarity F1 scores for various comparisons of generated texts and references.
        """
        print("Calculating BERT similarity F1 scores...")

        all_comparisons = [
            ('bert_prediction_edited', all_generated_texts, all_edited_endings),
            ('bert_prediction_cf', all_generated_texts, all_counterfactuals),
            ('bert_prediction_initial', all_generated_texts, all_initials),
            ('bert_prediction_original', all_generated_texts, all_original_endings),
            ('bert_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('bert_edited_ending_initial', all_edited_endings, all_initials),
            ('bert_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        bert_scores = {}
        for label, texts_a, texts_b in all_comparisons:
            if texts_b:
                try:
                    _, _, f1 = self.bert_scorer.score(texts_a, texts_b)
                    avg_f1 = f1.mean().item()
                    logger.info(f"{label}_f1: {avg_f1}")
                    bert_scores[f"{label}_f1"] = avg_f1
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bert_scores[f"{label}_f1"] = 'N/A'

        return bert_scores

    def calculate_and_log_bart_similarity(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs BART-based similarity scores for a variety of text comparisons,
        using the BARTScorer to evaluate the similarity between different segments of texts.
        """
        print("Calculating BART similarity scores...")
        
        all_comparisons = [
            ('bart_prediction_edited', all_generated_texts, all_edited_endings),
            ('bart_prediction_cf', all_generated_texts, all_counterfactuals),
            ('bart_prediction_initial', all_generated_texts, all_initials),
            ('bart_prediction_original', all_generated_texts, all_original_endings),
            ('bart_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('bart_edited_ending_initial', all_edited_endings, all_initials),
            ('bart_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        bart_scores = {}
        for label, src_texts, tgt_texts in all_comparisons:
            if tgt_texts:
                try:
                    scores = self.bart_scorer.score(src_texts, tgt_texts, batch_size=4)
                    avg_score = sum(scores) / len(scores) if scores else float('nan')
                    logger.info(f"{label}_avg_score: {avg_score}")
                    bart_scores[f"{label}_avg_score"] = avg_score
                    print(f"{label}_avg_score: {avg_score}")
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bart_scores[f"{label}_avg_score"] = 'N/A'
                    print(f"Error calculating {label}: {e}")

        return bart_scores
