import logging
from sacrebleu.metrics import BLEU
from rouge import Rouge
from bert_score import BERTScorer
from src.utils.config import CONFIG
import torch

logger = logging.getLogger(__name__)

class MetricsEvaluator:
    """
    A class for evaluating text generation models using various metrics such as BLEU, ROUGE, and BERTScore.
    This class helps compute rewards based on selected metrics for reinforcement learning methods like policy gradient.
    """

    def __init__(self):
        """
        Initializes the metric evaluators based on the configurations provided in CONFIG.
        Depending on the settings, it initializes evaluators for BLEU, ROUGE, and BERTScore.
        """
        print(f"Initializing MetricsEvaluator with config: {CONFIG}")
        
        # Initialize ROUGE for ROUGE score calculation
        self.rouge = Rouge()  

        # Initialize BERTScorer if configured to use BERTScore
        if CONFIG.get("use_bert", False):
            self.bert_scorer = BERTScorer(
                model_type=CONFIG["bert_scorer_model_type"],  # The BERT model type to use
                device='cuda:0',  # Ensure that all computations happen on GPU 0
                num_layers=None,  # Optionally specify number of layers; default to all layers
                batch_size=CONFIG["bert_scorer_batch_size"]  # Batch size for evaluation
            )
        else:
            self.bert_scorer = None  # Set bert_scorer to None if BERTScore is not used

        print("MetricsEvaluator initialized.")

    def calculate_reward(self, generated_texts, references):
        """
        Calculates the reward based on the specified metric in CONFIG.
        This method supports ROUGE-L and BERTScore as the reward mechanisms.
        
        Args:
            generated_texts (list of str): Texts generated by the model.
            references (list of str): Reference texts (ground truth).

        Returns:
            rewards (tensor): A tensor of reward values per example on GPU 0.
        """
        reward_metric = CONFIG.get("reward_metric", "rouge")

        print(f"Reward metric: {reward_metric}")  # Print the reward metric being used
        print(f"Generated texts: {generated_texts}")  # Print the generated texts
        print(f"Reference texts: {references}")  # Print the reference (ground truth) texts

        # Ensure both generated_texts and references are lists of strings
        generated_texts = [str(gt) for gt in generated_texts]
        references = [str(ref) for ref in references]

        # Case 1: ROUGE-L is selected as the reward metric
        if reward_metric == "rouge":
            # Calculate ROUGE-L F1 score between generated and reference texts
            print("Calculating ROUGE-L scores...")  # Indicate that ROUGE-L is being calculated
            rouge_scores = self.rouge.get_scores(generated_texts, references, avg=False)
            rewards = [score['rouge-l']['f'] for score in rouge_scores]  # List of rewards per example
            print(f"ROUGE-L scores: {rouge_scores}")  # Print the ROUGE scores
            print(f"ROUGE-L rewards: {rewards}")  # Print the ROUGE-L rewards

        # Case 2: BERTScore is selected as the reward metric
        elif reward_metric == "bert":
            if self.bert_scorer is None:
                raise ValueError("BERTScore is not initialized. Set 'use_bert' to True in CONFIG.")
            print("Calculating BERTScore...")  # Indicate that BERTScore is being calculated
            _, _, f1 = self.bert_scorer.score(generated_texts, references)
            rewards = f1.tolist()  # Convert tensor to list
            print(f"BERT F1 scores: {f1}")  # Print the BERT F1 scores
            print(f"BERT rewards: {rewards}")  # Print the BERTScore rewards

        # Raise an error if an unsupported reward metric is specified
        else:
            raise ValueError(f"Unsupported reward metric: {reward_metric}")

        # Convert the rewards to a tensor and move it to the correct device (GPU 0)
        rewards_tensor = torch.tensor(rewards, dtype=torch.float32).to('cuda:0')
        print(f"Rewards tensor (on GPU 0): {rewards_tensor}")  # Print the final rewards tensor
        return rewards_tensor


    def calculate_and_log_bleu_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs SacreBLEU scores for various comparisons between generated texts and references.
        
        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging BLEU score information

        Returns:
        - bleu_scores: Dictionary with calculated BLEU scores for different comparisons
        """
        print("Calculating BLEU scores...")

        # Prepare references for BLEU score calculation
        edited_endings_refs = [[ending] for ending in all_edited_endings] if all_edited_endings else None
        counterfactuals_refs = [[cf] for cf in all_counterfactuals]
        initials_refs = [[init] for init in all_initials]
        original_endings_refs = [[orig] for orig in all_original_endings]

        # List of all comparisons we want to calculate BLEU scores for
        all_comparisons = [
            ('bleu_prediction_edited', all_generated_texts, edited_endings_refs),
            ('bleu_prediction_cf', all_generated_texts, counterfactuals_refs),
            ('bleu_prediction_initial', all_generated_texts, initials_refs),
            ('bleu_prediction_original', all_generated_texts, original_endings_refs),
            ('bleu_edited_ending_cf', all_edited_endings, counterfactuals_refs),
            ('bleu_edited_ending_initial', all_edited_endings, initials_refs),
            ('bleu_edited_ending_original', all_edited_endings, original_endings_refs),
        ]

        # Dictionary to store BLEU scores for each comparison
        bleu_scores = {}
        for label, texts, references in all_comparisons:
            if references is not None:
                try:
                    # Calculate BLEU score
                    bleu_result = self.sacre_bleu.corpus_score(texts, references)
                    bleu_score = bleu_result.score
                    logger.info(f"{label}: {bleu_score}")
                    bleu_scores[label] = bleu_score
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bleu_scores[label] = 'N/A'

        return bleu_scores

    def calculate_and_log_rouge_scores(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs ROUGE scores for various comparisons between generated texts and references.

        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging ROUGE score information

        Returns:
        - rouge_scores: Dictionary with calculated ROUGE-L F1 scores for different comparisons
        """
        print("Calculating ROUGE scores...")

        # List of all comparisons we want to calculate ROUGE-L F1 scores for
        all_comparisons = [
            ('rouge_prediction_edited', all_generated_texts, all_edited_endings),
            ('rouge_prediction_cf', all_generated_texts, all_counterfactuals),
            ('rouge_prediction_initial', all_generated_texts, all_initials),
            ('rouge_prediction_original', all_generated_texts, all_original_endings),
            ('rouge_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('rouge_edited_ending_initial', all_edited_endings, all_initials),
            ('rouge_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        # Dictionary to store ROUGE-L F1 scores for each comparison
        rouge_scores = {}
        for label, hypotheses, references in all_comparisons:
            if references:
                try:
                    # Calculate ROUGE-L score
                    rouge_scores_set = self.rouge.get_scores(hypotheses, references, avg=True)
                    score_type = 'rouge-l'
                    rouge_scores[f"{label}_{score_type}_f"] = rouge_scores_set[score_type]['f']
                    logger.info(f"{label}_{score_type}_f: {rouge_scores_set[score_type]['f']}")
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    rouge_scores[f"{label}_f"] = 'N/A'

        return rouge_scores

    def calculate_and_log_bert_similarity(self, all_generated_texts, all_edited_endings, all_counterfactuals, all_initials, all_premises, all_original_endings, logger):
        """
        Calculates and logs BERT similarity F1 scores for various comparisons of generated texts and references.

        Args:
        - all_generated_texts: List of generated texts
        - all_edited_endings: List of reference edited endings
        - all_counterfactuals: List of counterfactuals
        - all_initials: List of initial events
        - all_premises: List of premises
        - all_original_endings: List of original endings
        - logger: Logger for logging BERTScore information

        Returns:
        - bert_scores: Dictionary with calculated BERT F1 scores for different comparisons
        """
        print("Calculating BERT similarity F1 scores...")

        all_comparisons = [
            ('bert_prediction_edited', all_generated_texts, all_edited_endings),
            ('bert_prediction_cf', all_generated_texts, all_counterfactuals),
            ('bert_prediction_initial', all_generated_texts, all_initials),
            ('bert_prediction_original', all_generated_texts, all_original_endings),
            ('bert_edited_ending_cf', all_edited_endings, all_counterfactuals),
            ('bert_edited_ending_initial', all_edited_endings, all_initials),
            ('bert_edited_ending_original', all_edited_endings, all_original_endings),
        ]

        # Dictionary to store BERT F1 scores for each comparison
        bert_scores = {}
        for label, texts_a, texts_b in all_comparisons:
            if texts_b:
                try:
                    # Calculate BERTScore F1
                    _, _, f1 = self.bert_scorer.score(texts_a, texts_b)
                    avg_f1 = f1.mean().item()
                    logger.info(f"{label}_f1: {avg_f1}")
                    bert_scores[f"{label}_f1"] = avg_f1
                except Exception as e:
                    logger.error(f"Error calculating {label}: {e}")
                    bert_scores[f"{label}_f1"] = 'N/A'

        return bert_scores
